{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_SUMMARY = False\n",
    "FIND_PDFS = False\n",
    "\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/Library/Java/JavaVirtualMachines/jdk-11.0.2.jdk/Contents/Home\"\n",
    "from pyserini.search import pysearch\n",
    "\n",
    "minDate = '2020/04/02'\n",
    "luceneDir = 'lucene-index-covid-2020-04-03/'\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import torch\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import numpy as np\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "QA_MODEL = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "QA_TOKENIZER = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "QA_MODEL.to(torch_device)\n",
    "QA_MODEL.eval()\n",
    "\n",
    "if USE_SUMMARY:\n",
    "    SUMMARY_TOKENIZER = BartTokenizer.from_pretrained('bart-large-cnn')\n",
    "    SUMMARY_MODEL = BartForConditionalGeneration.from_pretrained('bart-large-cnn')\n",
    "    SUMMARY_MODEL.to(torch_device)\n",
    "    SUMMARY_MODEL.eval()\n",
    "    \n",
    "query = 'Which non-pharmaceutical interventions limit tramsission'\n",
    "keywords = '2019-nCoV, SARS-CoV-2, COVID-19, non-pharmaceutical interventions, npi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "searcher = pysearch.SimpleSearcher(luceneDir)\n",
    "hits = searcher.search(query + '. ' + keywords)\n",
    "n_hits = len(hits)\n",
    "## collect the relevant data in a hit dictionary\n",
    "hit_dictionary = {}\n",
    "for i in range(0, n_hits):\n",
    "    doc_json = json.loads(hits[i].raw)\n",
    "    idx = str(hits[i].docid)\n",
    "    hit_dictionary[idx] = doc_json\n",
    "    hit_dictionary[idx]['title'] = hits[i].lucene_document.get(\"title\")\n",
    "    hit_dictionary[idx]['authors'] = hits[i].lucene_document.get(\"authors\")\n",
    "    hit_dictionary[idx]['doi'] = hits[i].lucene_document.get(\"doi\")\n",
    "\n",
    "## scrub the abstracts in prep for BERT-SQuAD\n",
    "for idx,v in hit_dictionary.items():\n",
    "    abs_dirty = v['abstract']\n",
    "    # looks like the abstract value can be an empty list\n",
    "    v['abstract_paragraphs'] = []\n",
    "    v['abstract_full'] = ''\n",
    "\n",
    "    if abs_dirty:\n",
    "        # looks like if it is a list, then the only entry is a dictionary wher text is in 'text' key\n",
    "        # looks like it is broken up by paragraph if it is in that form.  lets make lists for every paragraph\n",
    "        # and a new entry that is full abstract text as both could be valuable for BERT derrived QA\n",
    "\n",
    "\n",
    "        if isinstance(abs_dirty, list):\n",
    "            for p in abs_dirty:\n",
    "                v['abstract_paragraphs'].append(p['text'])\n",
    "                v['abstract_full'] += p['text'] + ' \\n\\n'\n",
    "\n",
    "        # looks like in some cases the abstract can be straight up text so we can actually leave that alone\n",
    "        if isinstance(abs_dirty, str):\n",
    "            v['abstract_paragraphs'].append(abs_dirty)\n",
    "            v['abstract_full'] += abs_dirty + ' \\n\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_txt = hit_dictionary['we62087x']['abstract_full']\n",
    "document = test_txt\n",
    "question = query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Which non-pharmaceutical interventions limit tramsission'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human infections with a novel coronavirus (SARS-CoV-2) were first identified via syndromic surveillance in December of 2019 in Wuhan China. Since identification, infections (coronavirus disease-2019; COVID-19) caused by this novel pathogen have spread globally, with more than 180,000 confirmed cases as of March 16, 2020. Effective public health interventions, including social distancing, contact tracing, and isolation/quarantine rely on the rapid and accurate identification of confirmed cases. However, testing capacity (having sufficient tests and laboratory throughput) to support these non-pharmaceutical interventions remains a challenge for containment and mitigation of COVID-19 infections. We undertook a sentinel event strategy (where single health events signal emerging trends) to estimate the incidence of COVID-19 in the US. Data from a recent national conference, the Conservative Political Action Conference, (CPAC) near Washington, DC and from the outbreak in Wuhan, China were used to fit a simple exponential growth model to estimate the total number of incident SARS-CoV-2 infections in the United States on March 1, 2020, and to forecast subsequent infections potentially undetected by current testing strategies. Our analysis and forecasting estimates a total of 54,100 SARS-CoV-2 infections (80 % CI 5,600 to 125,300) have occurred in the United States to March 12, 2020. Our forecast predicts that a very substantial number of infections are undetected, and without extensive and far-reaching non-pharmaceutical interventions, the number of infections should be expected to grow at an exponential rate. \\n\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_len = len(QA_TOKENIZER.encode(question))\n",
    "token_ls = QA_TOKENIZER.encode(document)\n",
    "def split_doc(text, question, tokenizer, overlap_rate):\n",
    "    token_ls = tokenizer.encode(text)\n",
    "    question_ls = tokenizer.encode(question)\n",
    "    question_len = len(question_ls)\n",
    "#     print(question_len)\n",
    "    piece_length = 500 - question_len\n",
    "    ret = []\n",
    "    start_idx = 0\n",
    "    while True:\n",
    "        start_idx = max(0, start_idx - int((overlap_rate-1)*piece_length))\n",
    "        end_idx = start_idx + piece_length\n",
    "        content = tokenizer.decode(token_ls[start_idx: end_idx])\n",
    "        content = content.replace('[CLS]', '').replace('[SEP]', '').strip()\n",
    "        ret.append(content)\n",
    "        start_idx = end_idx\n",
    "        if start_idx > len(token_ls):\n",
    "            break\n",
    "#     print([len(tokenizer.encode(con)) for con in ret])\n",
    "    return ret\n",
    "    \n",
    "        \n",
    "a = split_doc(test_txt, question, QA_TOKENIZER, overlap_rate=1.1)\n",
    "txt = QA_TOKENIZER.encode(question, a[0])\n",
    "# len(a[2].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344\n"
     ]
    }
   ],
   "source": [
    "def bert_pred(ipt_ids, tokenizer):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(ipt_ids)\n",
    "    sep_index = ipt_ids.index(QA_TOKENIZER.sep_token_id)\n",
    "    num_seg_a = sep_index + 1\n",
    "    num_seg_b = len(ipt_ids) - num_seg_a\n",
    "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "    assert len(segment_ids) == len(ipt_ids)\n",
    "    n_ids = len(segment_ids)\n",
    "    assert n_ids < 512\n",
    "    print(len(ipt_ids))\n",
    "    start_scores, end_scores = QA_MODEL(torch.tensor([ipt_ids]).to(torch_device), \n",
    "                                 token_type_ids=torch.tensor([segment_ids]).to(torch_device))\n",
    "    # Plan 1:\n",
    "#     answer_start, answer_end = torch.argmax(start_scores), \\\n",
    "#                                    torch.argmax(end_scores)\n",
    "    # Plan 2:\n",
    "\n",
    "    return start_scores, end_scores, {'tokens': tokens, 'sep_idx': sep_index}\n",
    "\n",
    "start_scores, end_scores, _ = bert_pred(txt, QA_TOKENIZER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_text(tokens, start=0, stop=-1):\n",
    "    tokens = tokens[start: stop]\n",
    "    if '[SEP]' in tokens:\n",
    "        sepind = tokens.index('[SEP]')\n",
    "        tokens = tokens[sepind+1:]\n",
    "    txt = ' '.join(tokens)\n",
    "    txt = txt.replace(' ##', '')\n",
    "    txt = txt.replace('##', '')\n",
    "    txt = txt.strip()\n",
    "    txt = \" \".join(txt.split())\n",
    "    txt = txt.replace(' .', '.')\n",
    "    txt = txt.replace('( ', '(')\n",
    "    txt = txt.replace(' )', ')')\n",
    "    txt = txt.replace(' - ', '-')\n",
    "    txt_list = txt.split(' , ')\n",
    "    txt = ''\n",
    "    nTxtL = len(txt_list)\n",
    "    if nTxtL == 1:\n",
    "        return txt_list[0]\n",
    "    newList =[]\n",
    "    for i,t in enumerate(txt_list):\n",
    "        if i < nTxtL -1:\n",
    "            if t[-1].isdigit() and txt_list[i+1][0].isdigit():\n",
    "                newList += [t,',']\n",
    "            else:\n",
    "                newList += [t, ', ']\n",
    "        else:\n",
    "            newList += [t]\n",
    "    answer = ''.join(newList) \n",
    "    if answer.startswith('. ') or answer.startswith(', '):\n",
    "            answer = answer[2:]\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plan 1, we get the best prediction for each piece\n",
    "# plan 2, we get the best a few predictions for whole article.\n",
    "def make_bert_squad_prediction(document, question):\n",
    "    overlap_rate = 1.1\n",
    "    doc_pieces = split_doc(document, question, QA_TOKENIZER, overlap_rate)\n",
    "    input_ids = [QA_TOKENIZER.encode(question, dp) for dp in doc_pieces] \n",
    "    print([len(i) for i in input_ids])\n",
    "    answers = []\n",
    "    confidences = []\n",
    "    for ipt_id in input_ids:\n",
    "        start_scores, end_scores, info = bert_pred(ipt_id, QA_TOKENIZER)\n",
    "        sep_index = info['sep_idx']\n",
    "        start_scores, end_scores = start_scores[:, sep_index:], \\\n",
    "                               end_scores[:, sep_index:]\n",
    "        tokens_wo_question = info['tokens'][sep_index:]\n",
    "\n",
    "        answer_start, answer_end = torch.argmax(start_scores), \\\n",
    "                                   torch.argmax(end_scores)\n",
    "        tokens = QA_TOKENIZER.convert_ids_to_tokens(ipt_id)\n",
    "        answer = reconstruct_text(tokens_wo_question, answer_start, answer_end+1)\n",
    "        total_score = start_scores[0,answer_start].item()+\\\n",
    "                      end_scores[0,answer_end].item()\n",
    "        answers.append(answer)\n",
    "        confidences.append(total_score)\n",
    "    max_conf = max(confidences)\n",
    "    argmax_conf = np.argmax(confidences)\n",
    "    max_answer = answers[argmax_conf]\n",
    "    return {'answer': answer,\n",
    "            'confidence': max_conf,\n",
    "            'text': document}\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "def search_abstracts(hit_dictionary, question):\n",
    "    result = OrderedDict()\n",
    "    for k,v in tqdm(hit_dictionary.items()):\n",
    "        abstract = v['abstract_full']\n",
    "        if abstract:\n",
    "            ans = make_bert_squad_prediction(abstract, question)\n",
    "            if ans['answer']: result[k]=ans\n",
    "    c_ls = np.array([result[key]['confidence'] for key in result])\n",
    "\n",
    "    if len(c_ls) != 0:\n",
    "        max_score = c_ls.max()\n",
    "        total = 0.0\n",
    "        exp_scores = np.exp(c_ls - max_score)\n",
    "        for i,k in enumerate(result):\n",
    "            result[k]['confidence'] = exp_scores[i]\n",
    "            \n",
    "    ret = {}\n",
    "    for k in result:\n",
    "        c = result[k]['confidence']\n",
    "        ret[c] = result[k].copy()\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[344]\n",
      "344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 1/10 [00:01<00:09,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500, 62]\n",
      "500\n",
      "62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 2/10 [00:03<00:10,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[240]\n",
      "240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:03<00:08,  1.17s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (634 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500, 209]\n",
      "500\n",
      "209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 4/10 [00:06<00:09,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[394]\n",
      "394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 5/10 [00:07<00:07,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[374]\n",
      "374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 6/10 [00:08<00:05,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[284]\n",
      "284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 7/10 [00:09<00:03,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[406]\n",
      "406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 8/10 [00:10<00:02,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[197]\n",
      "197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:11<00:01,  1.04s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (692 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500, 264]\n",
      "500\n",
      "264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:13<00:00,  1.40s/it]\n"
     ]
    }
   ],
   "source": [
    "answers = search_abstracts(hit_dictionary, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0007431192870043687: {'answer': 'medrxiv a license to display the preprint in perpetuity. is the (which was not peer-reviewed',\n",
       "  'confidence': 0.0007431192870043687,\n",
       "  'text': 'The COVID-19 outbreak containment strategies in China based on non-pharmaceutical interventions (NPIs) appear to be effective. Quantitative research is still needed however to assess the efficacy of different candidate NPIs and their timings to guide ongoing and future responses to epidemics of this emerging disease across the World. \\n\\nWe built a travel network-based susceptible-exposed-infectious-removed (SEIR) model to simulate the outbreak across cities in mainland China. We used epidemiological parameters estimated for the early stage of outbreak in Wuhan to parameterise the transmission before NPIs were implemented. To quantify the relative effect of various NPIs, daily changes of delay from illness onset to the first reported case in each county were used as a proxy for the improvement of case identification and isolation across the outbreak. Historical and near-real time human movement data, obtained from Baidu location-based service, were used to derive the intensity of travel restrictions and contact reductions across China. The model and outputs were validated using daily reported case numbers, with a series of sensitivity analyses conducted. \\n\\nWe estimated that there were a total of 114,325 COVID-19 cases (interquartile range [IQR] 76,776 -164,576) in mainland China as of February 29, 2020, and these were highly correlated (p<0.001, R 2 =0.86) with reported incidence. \\n\\nWithout NPIs, the number of COVID-19 cases would likely have shown a 67-fold increase (IQR: 44 -94), with the effectiveness of different interventions varying. The early detection and isolation of cases was estimated to prevent more infections than travel restrictions and contact reductions, but integrated NPIs would achieve the strongest and most rapid effect. If NPIs could have been conducted one week, two weeks, or three weeks earlier in China, cases could have been reduced by 66%, 86%, and 95%, respectively, together with significantly reducing the number of affected areas. However, if NPIs were conducted one week, two weeks, or three weeks later, the number of cases . CC-BY-NC-ND 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \\n\\nis the (which was not peer-reviewed) The copyright holder for this preprint . \\n\\n'},\n",
       " 0.00036367532313809987: {'answer': 'perceptions of 2009 h1n1 had nil effect on desire and intention',\n",
       "  'confidence': 0.00036367532313809987,\n",
       "  'text': \"2009 H1N1 Non-pharmaceutical intervention Model of goal-directed behavior Travel intention a b s t r a c t Theoretically, in the tourism context this study introduced a new concept of non-pharmaceutical intervention (NPI) for influenza, and tested the impact of NPI on the behavioral intention of potential international tourists. This study also extended the model of goal-directed behavior (MGB) by incorporating the new concepts of NPI, and the perception of 2009 H1N1. The model found that desire, perceived behavioral control, frequency of past behavior, and non-pharmaceutical interventions predicted tourists' intention but perceptions of 2009 H1N1 had nil effect on desire and intention. Personal non-pharmaceutical interventions were theorized as adaptive behavior of tourists intending to travel during a pandemic which should be supported by tourism operators on a system-wide basis. Practically, this study dealt with the issue of influenza 2009 H1N1 with the study findings and implications providing government agencies, tourism marketers, policy-makers, transport systems, and hospitality services with important suggestions for NPI and international tourism during pandemics. \\n\\n\"},\n",
       " 0.1570176534644779: {'answer': 'punishment by arrest',\n",
       "  'confidence': 0.1570176534644779,\n",
       "  'text': \"Non-pharmaceutical interventions (NPIs) are an important public health tool for responding to infectious disease outbreaks, including pandemics. However, little is known about the individual characteristics associated with support for NPIs, or whether they are consistent across regions. This study draws on survey data from four regions-Hong Kong, Singapore, Taiwan, and the United States-collected following the Severe Acute Respiratory Syndrome (SARS) outbreak of 2002-03, and employs regression techniques to estimate predictors of NPI support. It finds that characteristics associated with NPI support vary widely by region, possibly because of cultural variation and prior experience, and that minority groups tend to be less supportive of NPIs when arrest is the consequence of noncompliance. Prior experience of face-mask usage also results in increased support for future usage, as well as other NPIs. Policymakers should be attentive to local preferences and to the application of compulsory interventions. It is speculated here that some public health interventions may serve as 'gateway' exposures to future public health interventions. . Interviewees were selected through a combination of random-digit dialling (RDD) and telephone directory sampling, as was available in each region. Approximately 500 interviews were held in each region, and this information was weighted using a common weighting scheme, including age, income, number of people in the household, number of telephone lines in the household, and sex. The study sought and received Institutional Review Board approval, through the Harvard School of Public Health. \\n\\nFour major hypotheses regarding NPI prediction were developed: \\n\\n• Trust, particularly in public sources of information, will lead to increased support for the use of NPIs as well as punishment for non-compliance. \\n\\n• Concerns about infectious disease threats will lead to an increase in support for NPI utilisation and non-compliance punishment, while concerns about interventions will lead to a decrease in support. \\n\\n• Positive perceptions about the effectiveness of NPIs and prior experience of NPIs will lead to greater NPI support. \\n\\n• While levels of NPI support vary globally, the expectation is that this variation will be explained by demographic and experiential differences. Thus, the predictors of NPI support will be similar across regions. \\n\\nData were analysed using Stata 9 with survey analysis capabilities. Linear and logistic regression models were employed to establish the relationship between demographic and experiential factors and support for NPIs. The dependent variables (support for NPIs) were created from eight survey items. These eight items were grouped into three categories that were confirmed using factor analysis (see Appendix 1). The three categories were: support for basic NPIs; support for non-compliance punishment by arrest (that is, making an NPI mandatory); and quarantine location preferences (see Table 1 ). Ultimately, three main models were run for each of the four regions (see below). In addition, the same models were run for all four regions together, controlling for region, as well as for the three regions of Asia together, controlling for region. Also included are some results for identical models Pillemer et al. \\n\\n\"},\n",
       " 0.08236894902552677: {'answer': 'inadequate hand washing and household disinfection practices',\n",
       "  'confidence': 0.08236894902552677,\n",
       "  'text': \"Background: Non-pharmaceutical interventions (NPIs) constituted the principal public health response to the previous influenza A (H1N1) 2009 pandemic and are one key area of ongoing preparation for future pandemics. Thailand is an important point of focus in terms of global pandemic preparedness and response due to its role as the major transportation hub for Southeast Asia, the endemic presence of multiple types of influenza, and its role as a major receiving country for migrants. Our aim was to collect information about vulnerable migrants' perceptions of and ability to implement NPIs proposed by the WHO. We hope that this information will help us to gauge the capacity of this population to engage in pandemic preparedness and response efforts, and to identify potential barriers to NPI effectiveness. Methods: A cross-sectional survey was performed. The study was conducted during the influenza H1N1 2009 pandemic and included 801 migrant participants living in border areas thought to be high risk by the Thailand Ministry of Public Health. Data were collected by Migrant Community Health Workers using a 201-item interviewer-assisted questionnaire. Univariate descriptive analyses were conducted. Results: With the exception of border measures, to which nearly all participants reported they would be adherent, attitudes towards recommended NPIs were generally negative or uncertain. Other potential barriers to NPI implementation include limited experience applying these interventions (e.g., using a thermometer, wearing a face mask) and inadequate hand washing and household disinfection practices. Conclusions: Negative or ambivalent attitudes towards NPIs combined with other barriers identified suggest that vulnerable migrants in Thailand have a limited capacity to participate in pandemic preparedness efforts. This limited capacity likely puts migrants at risk of propagating the spread of a pandemic virus. Coordinated risk communication and public education are potential strategies that may reduce barriers to individual NPI implementation. \\n\\n\"},\n",
       " 0.3854110092118032: {'answer': 'social distancing measures',\n",
       "  'confidence': 0.3854110092118032,\n",
       "  'text': 'Much of the uncertainty about the progression of the COVID-19 pandemic stems from questions about when and how non-pharmaceutical interventions (NPI) by governments, in particular social distancing measures, are implemented, to what extent the population complies with these measures, and how compliance changes through time. Further uncertainty comes from a lack of knowledge of the potential effects of removing interventions once the epidemic is declining. By combining an epidemiological model of COVID-19 for the United Kingdom with simple sub-models for these societal processes, this study aims to shed light on the conceivable trajectories that the pandemic might follow over the next 1.5 years. We show strong improvements in outcomes if governments review NPI more frequently whereas, in comparison, the stability of compliance has surprisingly small effects on cumulative mortality. Assuming that mortality does considerably increase once a country\\'s hospital capacity is breached, we show that the inherent randomness of societal processes can lead to a wide range of possible outcomes, both in terms of disease dynamics and mortality, even when the principles according to which policy and population operate are identical.. Our model is easily modified to take other aspects of the socio-pandemic interaction into account. \\n\\nUsing a model based on the \"Susceptible -Infected -Recovered\" (SIR) framework with the infected population divided into infectious and noninfectious, and symptomatic and asymptomatic compartments to simulate 1 . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \\n\\n(which was not peer-reviewed) The copyright holder for this preprint . \\n\\n'},\n",
       " 0.2980922070199991: {'answer': 'virus spread by reducing contact between infected and susceptible persons',\n",
       "  'confidence': 0.2980922070199991,\n",
       "  'text': 'Background: Non-pharmaceutical interventions (NPI) are the first line of defense against pandemic influenza. These interventions dampen virus spread by reducing contact between infected and susceptible persons. Because they curtail essential societal activities, they must be applied judiciously. Optimal control theory is an approach for modeling and balancing competing objectives such as epidemic spread and NPI cost. Methods: We apply optimal control on an epidemiologic compartmental model to develop triggers for NPI implementation. The objective is to minimize expected person-days lost from influenza related deaths and NPI implementations for the model. We perform a multivariate sensitivity analysis based on Latin Hypercube Sampling to study the effects of input parameters on the optimal control policy. Additional studies investigated the effects of departures from the modeling assumptions, including exponential terminal time and linear NPI implementation cost. Results: An optimal policy is derived for the control model using a linear NPI implementation cost. Linear cost leads to a \"bang-bang\" policy in which NPIs are applied at maximum strength when certain state criteria are met. Multivariate sensitivity analyses are presented which indicate that NPI cost, death rate, and recovery rate are influential in determining the policy structure. Further death rate, basic reproductive number and recovery rate are the most influential in determining the expected cumulative death. When applying the NPI policy, the cumulative deaths under exponential and gamma terminal times are close, which implies that the outcome of applying the \"bang-bang\" policy is insensitive to the exponential assumption. Quadratic cost leads to a multi-level policy in which NPIs are applied at varying strength levels, again based on certain state criteria. Results indicate that linear cost leads to more costly implementation resulting in fewer deaths. Conclusions: The application of optimal control theory can provide valuable insight to developing effective control strategies for pandemic. Our findings highlight the importance of establishing a sensitive and timely surveillance system for pandemic preparedness. \\n\\n'},\n",
       " 1.0: {'answer': 'school closure, setting up a field hospital and community health education were implemented. these measures possibly limited the outbreak spreading to other schools nearby',\n",
       "  'confidence': 1.0,\n",
       "  'text': 'Non-pharmaceutical interventions are often recommended as a component of integrated control measures for pandemic influenza, but the effectiveness needs to be evaluated. An outbreak of influenza A (H1N1) in northern Thailand in November 2007 offered opportunity to evaluate these interventions. An investigation was conducted to describe the outbreak, evaluate effectiveness of non-pharmaceutical interventions and assess surge capacity of health agencies. A descriptive study was conducted by interviewing students and personnel in a school. We characterized transmission of the virus in this outbreak and explored effects of control measures. We identified that 44% of the students and teachers developed influenza during the 19-day outbreak. Non-pharmaceutical interventions including school closure, setting up a field hospital and community health education were implemented. These measures possibly limited the outbreak spreading to other schools nearby. Surveillance and preparedness plans could be strengthened to respond to pandemic and inter-pandemic influenza by using non-pharmaceutical interventions. \\n\\n'}}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BERT_func import BERT_SQUAD_QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[344]\n",
      "344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 1/10 [00:04<00:38,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500, 62]\n",
      "500\n",
      "62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 2/10 [00:06<00:28,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[240]\n",
      "240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:06<00:18,  2.69s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (634 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500, 209]\n",
      "500\n",
      "209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 4/10 [00:09<00:15,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[394]\n",
      "394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 5/10 [00:10<00:11,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[374]\n",
      "374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 6/10 [00:12<00:08,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[284]\n",
      "284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 7/10 [00:13<00:05,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[406]\n",
      "406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 8/10 [00:14<00:03,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[197]\n",
      "197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:15<00:01,  1.36s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (692 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500, 264]\n",
      "500\n",
      "264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:18<00:00,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0007431192870043687: {'answer': 'medrxiv a license to display the preprint in perpetuity. is the (which was not peer-reviewed', 'confidence': 0.0007431192870043687, 'text': 'The COVID-19 outbreak containment strategies in China based on non-pharmaceutical interventions (NPIs) appear to be effective. Quantitative research is still needed however to assess the efficacy of different candidate NPIs and their timings to guide ongoing and future responses to epidemics of this emerging disease across the World. \\n\\nWe built a travel network-based susceptible-exposed-infectious-removed (SEIR) model to simulate the outbreak across cities in mainland China. We used epidemiological parameters estimated for the early stage of outbreak in Wuhan to parameterise the transmission before NPIs were implemented. To quantify the relative effect of various NPIs, daily changes of delay from illness onset to the first reported case in each county were used as a proxy for the improvement of case identification and isolation across the outbreak. Historical and near-real time human movement data, obtained from Baidu location-based service, were used to derive the intensity of travel restrictions and contact reductions across China. The model and outputs were validated using daily reported case numbers, with a series of sensitivity analyses conducted. \\n\\nWe estimated that there were a total of 114,325 COVID-19 cases (interquartile range [IQR] 76,776 -164,576) in mainland China as of February 29, 2020, and these were highly correlated (p<0.001, R 2 =0.86) with reported incidence. \\n\\nWithout NPIs, the number of COVID-19 cases would likely have shown a 67-fold increase (IQR: 44 -94), with the effectiveness of different interventions varying. The early detection and isolation of cases was estimated to prevent more infections than travel restrictions and contact reductions, but integrated NPIs would achieve the strongest and most rapid effect. If NPIs could have been conducted one week, two weeks, or three weeks earlier in China, cases could have been reduced by 66%, 86%, and 95%, respectively, together with significantly reducing the number of affected areas. However, if NPIs were conducted one week, two weeks, or three weeks later, the number of cases . CC-BY-NC-ND 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \\n\\nis the (which was not peer-reviewed) The copyright holder for this preprint . \\n\\n'}, 0.00036367532313809987: {'answer': 'perceptions of 2009 h1n1 had nil effect on desire and intention', 'confidence': 0.00036367532313809987, 'text': \"2009 H1N1 Non-pharmaceutical intervention Model of goal-directed behavior Travel intention a b s t r a c t Theoretically, in the tourism context this study introduced a new concept of non-pharmaceutical intervention (NPI) for influenza, and tested the impact of NPI on the behavioral intention of potential international tourists. This study also extended the model of goal-directed behavior (MGB) by incorporating the new concepts of NPI, and the perception of 2009 H1N1. The model found that desire, perceived behavioral control, frequency of past behavior, and non-pharmaceutical interventions predicted tourists' intention but perceptions of 2009 H1N1 had nil effect on desire and intention. Personal non-pharmaceutical interventions were theorized as adaptive behavior of tourists intending to travel during a pandemic which should be supported by tourism operators on a system-wide basis. Practically, this study dealt with the issue of influenza 2009 H1N1 with the study findings and implications providing government agencies, tourism marketers, policy-makers, transport systems, and hospitality services with important suggestions for NPI and international tourism during pandemics. \\n\\n\"}, 0.1570176534644779: {'answer': 'punishment by arrest', 'confidence': 0.1570176534644779, 'text': \"Non-pharmaceutical interventions (NPIs) are an important public health tool for responding to infectious disease outbreaks, including pandemics. However, little is known about the individual characteristics associated with support for NPIs, or whether they are consistent across regions. This study draws on survey data from four regions-Hong Kong, Singapore, Taiwan, and the United States-collected following the Severe Acute Respiratory Syndrome (SARS) outbreak of 2002-03, and employs regression techniques to estimate predictors of NPI support. It finds that characteristics associated with NPI support vary widely by region, possibly because of cultural variation and prior experience, and that minority groups tend to be less supportive of NPIs when arrest is the consequence of noncompliance. Prior experience of face-mask usage also results in increased support for future usage, as well as other NPIs. Policymakers should be attentive to local preferences and to the application of compulsory interventions. It is speculated here that some public health interventions may serve as 'gateway' exposures to future public health interventions. . Interviewees were selected through a combination of random-digit dialling (RDD) and telephone directory sampling, as was available in each region. Approximately 500 interviews were held in each region, and this information was weighted using a common weighting scheme, including age, income, number of people in the household, number of telephone lines in the household, and sex. The study sought and received Institutional Review Board approval, through the Harvard School of Public Health. \\n\\nFour major hypotheses regarding NPI prediction were developed: \\n\\n• Trust, particularly in public sources of information, will lead to increased support for the use of NPIs as well as punishment for non-compliance. \\n\\n• Concerns about infectious disease threats will lead to an increase in support for NPI utilisation and non-compliance punishment, while concerns about interventions will lead to a decrease in support. \\n\\n• Positive perceptions about the effectiveness of NPIs and prior experience of NPIs will lead to greater NPI support. \\n\\n• While levels of NPI support vary globally, the expectation is that this variation will be explained by demographic and experiential differences. Thus, the predictors of NPI support will be similar across regions. \\n\\nData were analysed using Stata 9 with survey analysis capabilities. Linear and logistic regression models were employed to establish the relationship between demographic and experiential factors and support for NPIs. The dependent variables (support for NPIs) were created from eight survey items. These eight items were grouped into three categories that were confirmed using factor analysis (see Appendix 1). The three categories were: support for basic NPIs; support for non-compliance punishment by arrest (that is, making an NPI mandatory); and quarantine location preferences (see Table 1 ). Ultimately, three main models were run for each of the four regions (see below). In addition, the same models were run for all four regions together, controlling for region, as well as for the three regions of Asia together, controlling for region. Also included are some results for identical models Pillemer et al. \\n\\n\"}, 0.08236894902552677: {'answer': 'inadequate hand washing and household disinfection practices', 'confidence': 0.08236894902552677, 'text': \"Background: Non-pharmaceutical interventions (NPIs) constituted the principal public health response to the previous influenza A (H1N1) 2009 pandemic and are one key area of ongoing preparation for future pandemics. Thailand is an important point of focus in terms of global pandemic preparedness and response due to its role as the major transportation hub for Southeast Asia, the endemic presence of multiple types of influenza, and its role as a major receiving country for migrants. Our aim was to collect information about vulnerable migrants' perceptions of and ability to implement NPIs proposed by the WHO. We hope that this information will help us to gauge the capacity of this population to engage in pandemic preparedness and response efforts, and to identify potential barriers to NPI effectiveness. Methods: A cross-sectional survey was performed. The study was conducted during the influenza H1N1 2009 pandemic and included 801 migrant participants living in border areas thought to be high risk by the Thailand Ministry of Public Health. Data were collected by Migrant Community Health Workers using a 201-item interviewer-assisted questionnaire. Univariate descriptive analyses were conducted. Results: With the exception of border measures, to which nearly all participants reported they would be adherent, attitudes towards recommended NPIs were generally negative or uncertain. Other potential barriers to NPI implementation include limited experience applying these interventions (e.g., using a thermometer, wearing a face mask) and inadequate hand washing and household disinfection practices. Conclusions: Negative or ambivalent attitudes towards NPIs combined with other barriers identified suggest that vulnerable migrants in Thailand have a limited capacity to participate in pandemic preparedness efforts. This limited capacity likely puts migrants at risk of propagating the spread of a pandemic virus. Coordinated risk communication and public education are potential strategies that may reduce barriers to individual NPI implementation. \\n\\n\"}, 0.3854110092118032: {'answer': 'social distancing measures', 'confidence': 0.3854110092118032, 'text': 'Much of the uncertainty about the progression of the COVID-19 pandemic stems from questions about when and how non-pharmaceutical interventions (NPI) by governments, in particular social distancing measures, are implemented, to what extent the population complies with these measures, and how compliance changes through time. Further uncertainty comes from a lack of knowledge of the potential effects of removing interventions once the epidemic is declining. By combining an epidemiological model of COVID-19 for the United Kingdom with simple sub-models for these societal processes, this study aims to shed light on the conceivable trajectories that the pandemic might follow over the next 1.5 years. We show strong improvements in outcomes if governments review NPI more frequently whereas, in comparison, the stability of compliance has surprisingly small effects on cumulative mortality. Assuming that mortality does considerably increase once a country\\'s hospital capacity is breached, we show that the inherent randomness of societal processes can lead to a wide range of possible outcomes, both in terms of disease dynamics and mortality, even when the principles according to which policy and population operate are identical.. Our model is easily modified to take other aspects of the socio-pandemic interaction into account. \\n\\nUsing a model based on the \"Susceptible -Infected -Recovered\" (SIR) framework with the infected population divided into infectious and noninfectious, and symptomatic and asymptomatic compartments to simulate 1 . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \\n\\n(which was not peer-reviewed) The copyright holder for this preprint . \\n\\n'}, 0.2980922070199991: {'answer': 'virus spread by reducing contact between infected and susceptible persons', 'confidence': 0.2980922070199991, 'text': 'Background: Non-pharmaceutical interventions (NPI) are the first line of defense against pandemic influenza. These interventions dampen virus spread by reducing contact between infected and susceptible persons. Because they curtail essential societal activities, they must be applied judiciously. Optimal control theory is an approach for modeling and balancing competing objectives such as epidemic spread and NPI cost. Methods: We apply optimal control on an epidemiologic compartmental model to develop triggers for NPI implementation. The objective is to minimize expected person-days lost from influenza related deaths and NPI implementations for the model. We perform a multivariate sensitivity analysis based on Latin Hypercube Sampling to study the effects of input parameters on the optimal control policy. Additional studies investigated the effects of departures from the modeling assumptions, including exponential terminal time and linear NPI implementation cost. Results: An optimal policy is derived for the control model using a linear NPI implementation cost. Linear cost leads to a \"bang-bang\" policy in which NPIs are applied at maximum strength when certain state criteria are met. Multivariate sensitivity analyses are presented which indicate that NPI cost, death rate, and recovery rate are influential in determining the policy structure. Further death rate, basic reproductive number and recovery rate are the most influential in determining the expected cumulative death. When applying the NPI policy, the cumulative deaths under exponential and gamma terminal times are close, which implies that the outcome of applying the \"bang-bang\" policy is insensitive to the exponential assumption. Quadratic cost leads to a multi-level policy in which NPIs are applied at varying strength levels, again based on certain state criteria. Results indicate that linear cost leads to more costly implementation resulting in fewer deaths. Conclusions: The application of optimal control theory can provide valuable insight to developing effective control strategies for pandemic. Our findings highlight the importance of establishing a sensitive and timely surveillance system for pandemic preparedness. \\n\\n'}, 1.0: {'answer': 'school closure, setting up a field hospital and community health education were implemented. these measures possibly limited the outbreak spreading to other schools nearby', 'confidence': 1.0, 'text': 'Non-pharmaceutical interventions are often recommended as a component of integrated control measures for pandemic influenza, but the effectiveness needs to be evaluated. An outbreak of influenza A (H1N1) in northern Thailand in November 2007 offered opportunity to evaluate these interventions. An investigation was conducted to describe the outbreak, evaluate effectiveness of non-pharmaceutical interventions and assess surge capacity of health agencies. A descriptive study was conducted by interviewing students and personnel in a school. We characterized transmission of the virus in this outbreak and explored effects of control measures. We identified that 44% of the students and teachers developed influenza during the 19-day outbreak. Non-pharmaceutical interventions including school closure, setting up a field hospital and community health education were implemented. These measures possibly limited the outbreak spreading to other schools nearby. Surveillance and preparedness plans could be strengthened to respond to pandemic and inter-pandemic influenza by using non-pharmaceutical interventions. \\n\\n'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "QA_model = BERT_SQUAD_QA(QA_TOKENIZER, QA_MODEL)\n",
    "print(QA_model.search_abstracts(hit_dictionary, query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workingPath = './kaggle/working'\n",
    "import pandas as pd\n",
    "if FIND_PDFS:\n",
    "    from metapub import UrlReverse\n",
    "    from metapub import FindIt\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "#from summarizer import Summarizer\n",
    "#summarizerModel = Summarizer()\n",
    "def displayResults(hit_dictionary, answers, question):\n",
    "    \n",
    "    question_HTML = '<div style=\"font-family: Times New Roman; font-size: 28px; padding-bottom:28px\"><b>Query</b>: '+question+'</div>'\n",
    "    #all_HTML_txt = question_HTML\n",
    "    confidence = list(answers.keys())\n",
    "    confidence.sort(reverse=True)\n",
    "    \n",
    "    confidence = list(answers.keys())\n",
    "    confidence.sort(reverse=True)\n",
    "    \n",
    "\n",
    "    for c in confidence:\n",
    "        if c>0 and c <= 1 and len(answers[c]['answer']) != 0:\n",
    "            if 'idx' not in  answers[c]:\n",
    "                continue\n",
    "            rowData = []\n",
    "            idx = answers[c]['idx']\n",
    "            title = hit_dictionary[idx]['title']\n",
    "            authors = hit_dictionary[idx]['authors'] + ' et al.'\n",
    "            doi = '<a href=\"https://doi.org/'+hit_dictionary[idx]['doi']+'\" target=\"_blank\">' + title +'</a>'\n",
    "\n",
    "            \n",
    "            full_abs = answers[c]['abstract_bert']\n",
    "            bert_ans = answers[c]['answer']\n",
    "            \n",
    "            \n",
    "            split_abs = full_abs.split(bert_ans)\n",
    "            sentance_beginning = split_abs[0][split_abs[0].rfind('.')+1:]\n",
    "            if len(split_abs) == 1:\n",
    "                sentance_end_pos = len(full_abs)\n",
    "                sentance_end =''\n",
    "            else:\n",
    "                sentance_end_pos = split_abs[1].find('. ')+1\n",
    "                if sentance_end_pos == 0:\n",
    "                    sentance_end = split_abs[1]\n",
    "                else:\n",
    "                    sentance_end = split_abs[1][:sentance_end_pos]\n",
    "                \n",
    "            #sentance_full = sentance_beginning + bert_ans+ sentance_end\n",
    "            answers[c]['full_answer'] = sentance_beginning+bert_ans+sentance_end\n",
    "            answers[c]['sentence_beginning'] = sentance_beginning\n",
    "            answers[c]['sentence_end'] = sentance_end\n",
    "            answers[c]['title'] = title\n",
    "            answers[c]['doi'] = doi\n",
    "        else:\n",
    "            answers.pop(c)\n",
    "    \n",
    "    \n",
    "    ## now rerank based on semantic similarity of the answers to the question\n",
    "    cList = list(answers.keys())\n",
    "    allAnswers = [answers[c]['full_answer'] for c in cList]\n",
    "    \n",
    "    messages = [question]+allAnswers\n",
    "    \n",
    "    encoding_matrix = embed_fn(messages)\n",
    "    similarity_matrix = np.inner(encoding_matrix, encoding_matrix)\n",
    "    rankings = similarity_matrix[1:,0]\n",
    "    \n",
    "    for i,c in enumerate(cList):\n",
    "        answers[rankings[i]] = answers.pop(c)\n",
    "\n",
    "    ## now form pandas dv\n",
    "    confidence = list(answers.keys())\n",
    "    confidence.sort(reverse=True)\n",
    "    pandasData = []\n",
    "    ranked_aswers = []\n",
    "    for c in confidence:\n",
    "        rowData=[]\n",
    "        title = answers[c]['title']\n",
    "        doi = answers[c]['doi']\n",
    "        idx = answers[c]['idx']\n",
    "        rowData += [idx]            \n",
    "        sentance_html = '<div>' +answers[c]['sentence_beginning'] + \" <font color='red'>\"+answers[c]['answer']+\"</font> \"+answers[c]['sentence_end']+'</div>'\n",
    "        \n",
    "        rowData += [sentance_html, c, doi]\n",
    "        pandasData.append(rowData)\n",
    "        ranked_aswers.append(' '.join([answers[c]['full_answer']]))\n",
    "    \n",
    "    if FIND_PDFS:\n",
    "        pdata2 = []\n",
    "        for rowData in pandasData:\n",
    "            rd = rowData\n",
    "            idx = rowData[0]\n",
    "            if str(idx).startswith('pm_'):\n",
    "                pmid = idx[3:]\n",
    "            else:\n",
    "                try:\n",
    "                    test = UrlReverse('https://doi.org/'+hit_dictionary[idx]['doi'])\n",
    "                    if test is not None:\n",
    "                        pmid = test.pmid\n",
    "                    else:\n",
    "                        pmid = None\n",
    "                except:\n",
    "                    pmid = None\n",
    "            pdfLink = None\n",
    "            if pmid is not None:\n",
    "                try:\n",
    "                    pdfLink = FindIt(str(pmid))\n",
    "                except:\n",
    "                    pdfLink = None\n",
    "            if pdfLink is not None:\n",
    "                pdfLink = pdfLink.url\n",
    "\n",
    "            if pdfLink is None:\n",
    "\n",
    "                rd += ['Not Available']\n",
    "            else:\n",
    "                rd += ['<a href=\"'+pdfLink+'\" target=\"_blank\">PDF Link</a>']\n",
    "            pdata2.append(rowData)\n",
    "    else:\n",
    "        pdata2 = pandasData\n",
    "        \n",
    "    \n",
    "    display(HTML(question_HTML))\n",
    "    \n",
    "    if USE_SUMMARY:\n",
    "        ## try generating an exacutive summary with extractive summarizer\n",
    "        allAnswersTxt = ' '.join(ranked_aswers[:6]).replace('\\n','')\n",
    "    #    exec_sum = summarizerModel(allAnswersTxt, min_length=1, max_length=500)    \n",
    "     #   execSum_HTML = '<div style=\"font-family: Times New Roman; font-size: 18px; padding-bottom:18px\"><b>BERT Extractive Summary:</b>: '+exec_sum+'</div>'\n",
    "\n",
    "        answers_input_ids = SUMMARY_TOKENIZER.batch_encode_plus([allAnswersTxt], return_tensors='pt', max_length=1024)['input_ids'].to(torch_device)\n",
    "        summary_ids = SUMMARY_MODEL.generate(answers_input_ids,\n",
    "                                               num_beams=10,\n",
    "                                               length_penalty=1.2,\n",
    "                                               max_length=1024,\n",
    "                                               min_length=64,\n",
    "                                               no_repeat_ngram_size=4)\n",
    "\n",
    "        exec_sum = SUMMARY_TOKENIZER.decode(summary_ids.squeeze(), skip_special_tokens=True)\n",
    "        execSum_HTML = '<div style=\"font-family: Times New Roman; font-size: 18px; margin-bottom:1pt\"><b>BART Abstractive Summary:</b>: '+exec_sum+'</div>'\n",
    "        display(HTML(execSum_HTML))\n",
    "        warning_HTML = '<div style=\"font-family: Times New Roman; font-size: 12px; padding-bottom:12px; color:#CCCC00; margin-top:1pt\"> Warning this is an autogenerated summary based on semantic search of abstracts, always examine the sources before accepting this conclusion.  If the evidence only mentions topic in passing or the evidence is not clear, the summary will likely not clearly answer the question.</div>'\n",
    "        display(HTML(warning_HTML))\n",
    "\n",
    "#    display(HTML('<div style=\"font-family: Times New Roman; font-size: 18px; padding-bottom:18px\"><b>Body of Evidence:</b></div>'))\n",
    "    \n",
    "    if FIND_PDFS:\n",
    "        df = pd.DataFrame(pdata2, columns = ['Lucene ID', 'BERT-SQuAD Answer with Highlights', 'Confidence', 'Title/Link','PDF Link'])\n",
    "    else:\n",
    "        df = pd.DataFrame(pdata2, columns = ['Lucene ID', 'BERT-SQuAD Answer with Highlights', 'Confidence', 'Title/Link'])\n",
    "        \n",
    "    display(HTML(df.to_html(render_links=True, escape=False)))\n",
    "    \n",
    "displayResults(hit_dictionary, answers, query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('cord': conda)",
   "language": "python",
   "name": "python37764bitcordcondae067bba033284fdea4e8999502521686"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
